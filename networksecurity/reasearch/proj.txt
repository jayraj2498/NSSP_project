#Network Security proj  
1) first  we add the git setup ,requirement.txt , and then setup.py  

2) we define logging and exception handling code  

project : - 

1) data ingestion pre step : we are taking from mongodb  
    Q .how initial  data is coming to the mongodb database --? ETL Pipeline 

    ETL-> extract ,transform, load this work is done by data engg 

    we extract the data from various source ---> API , s3 bucket , Paid api , variuous source 
    we transform it ---> , clean it , preprocess it , in json form 
    we load it --> we load it various sorces like mongodb , aws , mysql , azure ...  



# python -m pip install "pymongo[srv]==3.6"
# mongodb+srv://jayraj2498:<db_password>@cluster0.a78lv.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0 

we insert our data to mongodb 

Inside utils.py we write all imp generic code def for reutillization purpose 
utils-> main_utils-> utils.py 

A) data ingestion : 
 in data ingesion their are 3 main component  data ingestion config ,  data ingestion component ,  data ingestion artifact 

we go networksecurity 
    
    constant -> training_pipiline -> init.py  we write all constant variable here 
    entity -> config_entity.py : we updat the data ingestion config 
    entity -> artifact_entity -> the output of data ingestion we are return it inside local dir
    component-> data_ingestion.py 

    the output of the data ingestion component it train-test   

    create main.py 
    

B) A) data ingestion : 
 wer get the data from from mongodb we make sure that our schema(features)  should not have to change 
 we need to check if data follow normal dis but after some times if our data not able to follow normal dis then this called data drift so we shold create data drift report 

here data ingestion artifact is given as input here 


    constant -> training_pipiline -> init.py  we write all constant variable here 
    data_schema-> make schema.yaml  for data drift 
    component-> data_validation.py