#Network Security proj  
1) first  we add the git setup ,requirement.txt , and then setup.py  

2) we define logging and exception handling code  

project : - 

1) data ingestion pre step : we are taking from mongodb  
    Q .how initial  data is coming to the mongodb database --? ETL Pipeline 

    ETL-> extract ,transform, load this work is done by data engg 

    we extract the data from various source ---> API , s3 bucket , Paid api , variuous source 
    we transform it ---> , clean it , preprocess it , in json form 
    we load it --> we load it various sorces like mongodb , aws , mysql , azure ...  



# python -m pip install "pymongo[srv]==3.6"
# mongodb+srv://jayraj2498:<db_password>@cluster0.a78lv.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0 

we insert our data to mongodb 

Inside utils.py we write all imp generic code def for reutillization purpose 
utils-> main_utils-> utils.py 

A) data ingestion : 
 in data ingesion their are 3 main component  data ingestion config ,  data ingestion component ,  data ingestion artifact 

we go networksecurity 
    
    constant -> training_pipiline -> init.py  we write all constant variable here 
    entity -> config_entity.py : we updat the data ingestion config 
    entity -> artifact_entity -> the output of data ingestion we are return it inside local dir
    component-> data_ingestion.py 

    the output of the data ingestion component it train-test   

    create main.py 
    

B)  data Validaiton : 
 wer get the data from from mongodb we make sure that our schema(features)  should not have to change 
 we need to check if data follow normal dis but after some times if our data not able to follow normal dis then this called data drift so we shold create data drift report 

here data ingestion artifact is given as input here 


    constant -> training_pipiline -> init.py  we write all constant variable here 
    data_schema-> make schema.yaml  for data drift 
    component-> data_validation.py 


C)  data Transformation : 
    we do features enginering here :replace nam val , herer we gonna create pickle file so it can be use anywhere 
    All infrmation coming from data valiation artifact is the input for data transfomation 
    once we we read tain , test data then we move ahed for data transformation step 
    inside our data their is some nan val we try to replace it also 
    we use amotetomek liabrary for balancing the data if their is imbalance then we use it  

    update config_entity & artifact entity 
    update constant var inside training_pipeline conatnt 
    component -> make data_transformation.py update code 


D) Model traning :
    we have done data_transformation artifact we will providde input to it 
    we first create model trainer config it will have details after training the model where we should save it in which folder 
    the op of it is model trainer artifact
    here we create the model.pkl file 
    whatever model we get best score we save it as our best model 
    we make : E:\Ds_p1\ncp\networksecurity\utils\ml_utils\metric
    we make : E:\Ds_p1\ncp\networksecurity\utils\ml_utils\model
